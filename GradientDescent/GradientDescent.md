This Notebook dives deep into the core concepts of deep learning, backpropagation, and automatic differentiation. This simple implementation provides an intuitive way to learn how neural networks are trained by computing gradients and applying optimization algorithms like gradient descent.

* **Understand the core concepts of backpropagation**: Learn how gradients are computed and how they propagate through a network.
* **Learn about gradient descent**: See how optimization algorithms update model parameters during training.
* **Gain insight into neural network internals**: Explore how neural networks are built and how they learn from data.
* **Access a simple, readable codebase**: With just a few hundred lines of Python code, you can easily follow and modify the core algorithms.

Inspired by - [Andrej Karpathy](https://youtu.be/VMj-3S1tku0?si=EE2T3ts-ROyhO1Ig)

Java Implementation

In addition to the Python notebook, I have implemented the core concepts and algorithms from the video in Java. This Java version mirrors the same principles of backpropagation, automatic differentiation, and gradient descent, providing an alternative perspective for those interested in exploring these fundamentals in a statically typed language.

You can find the Java implementation in the /Java Impementation directory. Feel free to explore, modify, and extend it!